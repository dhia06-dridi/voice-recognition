{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f8a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Using device: cuda\n",
      "üìÅ Checking data...\n",
      "‚úÖ Data folder found: D:\\voice_processing\\data\\SpeechCommands\\speech_commands_v0.02\n",
      "üéØ Detected classes: 35\n",
      "üìù Classes: ['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward']...\n",
      "‚öôÔ∏è Model Configuration:\n",
      "   input_dropout: 0.1\n",
      "   feature_dropout: 0.4\n",
      "   classifier_dropout: 0.3\n",
      "   weight_decay: 0.01\n",
      "   label_smoothing: 0.1\n",
      "   learning_rate: 0.001\n",
      "   batch_size: 16\n",
      "   num_epochs: 100\n",
      "   n_mels: 128\n",
      "   n_fft: 1024\n",
      "   hop_length: 256\n",
      "   sample_rate: 16000\n",
      "   target_length: 16000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports et configuration\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import timm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration optimis√©e\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"üî• Using device: {device}\")\n",
    "\n",
    "# V√©rification des donn√©es\n",
    "print(\"üìÅ Checking data...\")\n",
    "data_path = r\"D:\\voice_processing\\data\\SpeechCommands\\speech_commands_v0.02\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"‚úÖ Data folder found: {data_path}\")\n",
    "    classes = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d)) and not d.startswith('_')]\n",
    "    print(f\"üéØ Detected classes: {len(classes)}\")\n",
    "    print(f\"üìù Classes: {classes[:10]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Data folder not found! Automatic download will occur...\")\n",
    "\n",
    "# Configuration du mod√®le\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        # Dropout configuration\n",
    "        self.input_dropout = 0.1\n",
    "        self.feature_dropout = 0.4\n",
    "        self.classifier_dropout = 0.3\n",
    "        \n",
    "        # Training configuration\n",
    "        self.weight_decay = 0.01\n",
    "        self.label_smoothing = 0.1\n",
    "        self.learning_rate = 1e-3\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 100  # Augment√© pour permettre √† l'early stopping de fonctionner\n",
    "        \n",
    "        # Audio configuration\n",
    "        self.n_mels = 128\n",
    "        self.n_fft = 1024\n",
    "        self.hop_length = 256\n",
    "        self.sample_rate = 16000\n",
    "        self.target_length = 16000  # 1 second\n",
    "\n",
    "config = ModelConfig()\n",
    "print(\"‚öôÔ∏è Model Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e3e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Early Stopping am√©lior√© et Augmentation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.best_acc = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, val_loss, val_acc, model, epoch):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_acc = val_acc\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"üîÑ EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(\"üõë Early stopping triggered!\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_acc = val_acc\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            print(\"‚úÖ New best model saved!\")\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def restore_best_model(self, model):\n",
    "        if self.restore_best_weights and self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "            print(f\"‚úÖ Best weights restored from epoch {self.best_epoch}!\")\n",
    "            print(f\"üèÜ Best validation accuracy: {self.best_acc:.2f}%\")\n",
    "\n",
    "class AudioAugmentation:\n",
    "    def __init__(self):\n",
    "        self.time_stretch = T.TimeStretch()\n",
    "        self.pitch_shift = T.PitchShift(sample_rate=config.sample_rate, n_steps=4)\n",
    "        \n",
    "    def __call__(self, waveform):\n",
    "        # Time stretching\n",
    "        if random.random() > 0.6:\n",
    "            rate = random.uniform(0.85, 1.15)\n",
    "            try:\n",
    "                waveform = self.time_stretch(waveform, rate)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Pitch shifting\n",
    "        if random.random() > 0.6:\n",
    "            try:\n",
    "                waveform = self.pitch_shift(waveform)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Gaussian noise\n",
    "        if random.random() > 0.7:\n",
    "            noise = torch.randn_like(waveform) * 0.005\n",
    "            waveform = waveform + noise\n",
    "        \n",
    "        # Random gain\n",
    "        if random.random() > 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            waveform = waveform * gain\n",
    "            \n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05032be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset et DataLoader (MODIFI√â)\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, subset='training', apply_augmentation=False):\n",
    "        # Utilisez votre chemin local existant au lieu de t√©l√©charger\n",
    "        self.data_path = r\"D:\\voice_processing\\data\\SpeechCommands\\speech_commands_v0.02\"\n",
    "        \n",
    "        # Chargez manuellement les fichiers selon le subset\n",
    "        if subset == 'training':\n",
    "            # Pour l'entra√Ænement, utilisez tous les fichiers SAUF ceux dans les listes de validation/test\n",
    "            with open(os.path.join(self.data_path, 'validation_list.txt'), 'r') as f:\n",
    "                val_files = set(f.read().splitlines())\n",
    "            with open(os.path.join(self.data_path, 'testing_list.txt'), 'r') as f:\n",
    "                test_files = set(f.read().splitlines())\n",
    "            \n",
    "            all_files = []\n",
    "            for class_name in os.listdir(self.data_path):\n",
    "                class_path = os.path.join(self.data_path, class_name)\n",
    "                if os.path.isdir(class_path) and not class_name.startswith('_'):\n",
    "                    for file in os.listdir(class_path):\n",
    "                        if file.endswith('.wav'):\n",
    "                            rel_path = os.path.join(class_name, file)\n",
    "                            if rel_path not in val_files and rel_path not in test_files:\n",
    "                                all_files.append((os.path.join(class_path, file), class_name))\n",
    "            \n",
    "            self.samples = all_files\n",
    "            \n",
    "        elif subset == 'validation':\n",
    "            # Pour la validation, utilisez validation_list.txt\n",
    "            with open(os.path.join(self.data_path, 'validation_list.txt'), 'r') as f:\n",
    "                val_files = f.read().splitlines()\n",
    "            \n",
    "            self.samples = []\n",
    "            for rel_path in val_files:\n",
    "                class_name = rel_path.split('/')[0]\n",
    "                full_path = os.path.join(self.data_path, rel_path)\n",
    "                if os.path.exists(full_path):\n",
    "                    self.samples.append((full_path, class_name))\n",
    "                    \n",
    "        elif subset == 'testing':\n",
    "            # Pour le test, utilisez testing_list.txt\n",
    "            with open(os.path.join(self.data_path, 'testing_list.txt'), 'r') as f:\n",
    "                test_files = f.read().splitlines()\n",
    "            \n",
    "            self.samples = []\n",
    "            for rel_path in test_files:\n",
    "                class_name = rel_path.split('/')[0]\n",
    "                full_path = os.path.join(self.data_path, rel_path)\n",
    "                if os.path.exists(full_path):\n",
    "                    self.samples.append((full_path, class_name))\n",
    "        \n",
    "        # R√©cup√©rer toutes les classes\n",
    "        self.labels = sorted(list(set([label for _, label in self.samples])))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.augment = AudioAugmentation()\n",
    "        \n",
    "        print(f\"üìä {subset}: {len(self.samples)} samples, {len(self.labels)} classes\")\n",
    "        \n",
    "        # Transformations audio (garder le reste du code inchang√©)\n",
    "        self.transform = T.MelSpectrogram(\n",
    "            sample_rate=config.sample_rate,\n",
    "            n_fft=config.n_fft,\n",
    "            hop_length=config.hop_length,\n",
    "            n_mels=config.n_mels,\n",
    "            f_min=20,\n",
    "            f_max=8000\n",
    "        )\n",
    "        self.to_db = T.AmplitudeToDB()\n",
    "        \n",
    "        # SpecAugment\n",
    "        self.time_mask = T.TimeMasking(time_mask_param=20)\n",
    "        self.freq_mask = T.FrequencyMasking(freq_mask_param=10)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        \n",
    "        # Charger l'audio\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Resampling\n",
    "        if sample_rate != config.sample_rate:\n",
    "            resampler = T.Resample(sample_rate, config.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Le reste du code __getitem__ reste inchang√©...\n",
    "        # Normalisation\n",
    "        waveform = waveform / (waveform.abs().max() + 1e-8)\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.apply_augmentation and random.random() > 0.5:\n",
    "            waveform = self.augment(waveform)\n",
    "        \n",
    "        # Padding/truncation\n",
    "        current_length = waveform.shape[1]\n",
    "        if current_length < config.target_length:\n",
    "            waveform = F.pad(waveform, (0, config.target_length - current_length))\n",
    "        else:\n",
    "            if self.apply_augmentation:\n",
    "                start = random.randint(0, current_length - config.target_length)\n",
    "            else:\n",
    "                start = (current_length - config.target_length) // 2\n",
    "            waveform = waveform[:, start:start + config.target_length]\n",
    "        \n",
    "        # Mel Spectrogram\n",
    "        mel_spec = self.transform(waveform)\n",
    "        mel_spec_db = self.to_db(mel_spec)\n",
    "        \n",
    "        # SpecAugment pour l'entra√Ænement\n",
    "        if self.apply_augmentation:\n",
    "            mel_spec_db = self.time_mask(mel_spec_db)\n",
    "            mel_spec_db = self.freq_mask(mel_spec_db)\n",
    "        \n",
    "        # Normalisation\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "        \n",
    "        # 3 channels pour MobileNetV3\n",
    "        mel_spec_3ch = mel_spec_db.repeat(3, 1, 1)\n",
    "        \n",
    "        label_idx = self.label_to_idx[label]\n",
    "        \n",
    "        return mel_spec_3ch, label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dace8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Version alternative plus simple\n",
    "class MobileNetV3AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=35, freeze_layers=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone MobileNetV3 Large pr√©-entra√Æn√©\n",
    "        self.backbone = timm.create_model(\n",
    "            'mobilenetv3_large_100',\n",
    "            pretrained=True,\n",
    "            in_chans=3,\n",
    "            num_classes=num_classes  # Utiliser le classificateur original\n",
    "        )\n",
    "        \n",
    "        # Geler les premi√®res couches\n",
    "        self._freeze_layers(freeze_layers)\n",
    "        \n",
    "        print(f\"‚úÖ MobileNetV3 Large cr√©√© avec {freeze_layers} couches gel√©es\")\n",
    "        \n",
    "    def _freeze_layers(self, num_layers):\n",
    "        \"\"\"Geler les premi√®res couches du backbone\"\"\"\n",
    "        layers_frozen = 0\n",
    "        \n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'conv' in name or 'blocks' in name:\n",
    "                if layers_frozen < num_layers:\n",
    "                    param.requires_grad = False\n",
    "                    layers_frozen += 1\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        print(f\"‚ùÑÔ∏è {layers_frozen} couches gel√©es\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02b9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Fonctions de perte et entra√Ænement\n",
    "class LabelSmoothCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entra√Ænement pour une epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='üöÄ Training')\n",
    "    for data, target in pbar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Accuracy': f'{accuracy:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(loader, desc='üìä Validation'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d46539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Chargement des datasets...\n",
      "üìä training: 105829 samples, 35 classes\n",
      "üìä validation: 9981 samples, 35 classes\n",
      "üìä testing: 11005 samples, 35 classes\n",
      "‚úÖ Donn√©es charg√©es avec succ√®s!\n",
      "üìä R√©sum√©:\n",
      "   - Mode: RAPIDE (Test)\n",
      "   - Batch size: 32\n",
      "   - √âchantillons d'entra√Ænement: 105,829\n",
      "   - Classes: 35\n",
      "‚ùÑÔ∏è 5 couches gel√©es\n",
      "‚úÖ MobileNetV3 Large cr√©√© avec 5 couches gel√©es\n",
      "\n",
      "üìä Informations du mod√®le:\n",
      "   - Param√®tres totaux: 4,246,867\n",
      "   - Param√®tres entra√Ænables: 4,246,003\n",
      "   - Param√®tres gel√©s: 864\n",
      "   - Pourcentage gel√©: 0.0%\n",
      "   - Taux de dropout: 0.4\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Chargement des donn√©es et cr√©ation du mod√®le\n",
    "print(\"üîß Chargement des datasets...\")\n",
    "\n",
    "# Mode rapide pour les tests\n",
    "FAST_MODE = True\n",
    "SUBSET_SIZE = 5000 if FAST_MODE else None\n",
    "\n",
    "# Chargement des datasets\n",
    "train_dataset = SpeechCommandsDataset(subset='training', apply_augmentation=True)\n",
    "val_dataset = SpeechCommandsDataset(subset='validation', apply_augmentation=False)\n",
    "test_dataset = SpeechCommandsDataset(subset='testing', apply_augmentation=False)\n",
    "\n",
    "num_classes = len(train_dataset.labels)\n",
    "\n",
    "# Ajustement de la batch size selon le mode\n",
    "batch_size = 32 if FAST_MODE else config.batch_size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(\"‚úÖ Donn√©es charg√©es avec succ√®s!\")\n",
    "print(f\"üìä R√©sum√©:\")\n",
    "print(f\"   - Mode: {'RAPIDE (Test)' if FAST_MODE else 'COMPLET (Entra√Ænement)'}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - √âchantillons d'entra√Ænement: {len(train_dataset):,}\")\n",
    "print(f\"   - Classes: {num_classes}\")\n",
    "\n",
    "# Cr√©ation du mod√®le avec gel des 5 premi√®res couches\n",
    "model = MobileNetV3AudioClassifier(num_classes=num_classes, freeze_layers=5).to(device)\n",
    "\n",
    "# Affichage des informations du mod√®le\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nüìä Informations du mod√®le:\")\n",
    "print(f\"   - Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"   - Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"   - Param√®tres gel√©s: {frozen_params:,}\")\n",
    "print(f\"   - Pourcentage gel√©: {frozen_params/total_params*100:.1f}%\")\n",
    "print(f\"   - Taux de dropout: {config.feature_dropout}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834ddb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage de l'entra√Ænement avec MobileNetV3...\n",
      "\n",
      "üéØ D√©marrage de l'entra√Ænement pour 100 epochs...\n",
      "‚è∞ Early Stopping: Patience = 10 epochs\n",
      "\n",
      "üìç Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÄ Training:   0%|          | 9/3308 [00:53<5:27:34,  5.96s/it, Loss=4.7712, Accuracy=3.82%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìç Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Phase d'entra√Ænement\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Phase de validation\u001b[39;00m\n\u001b[0;32m     41\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32me:\\mfe_proggramation\\pytorche_windows\\pytorche_windows\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\mfe_proggramation\\pytorche_windows\\pytorche_windows\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\mfe_proggramation\\pytorche_windows\\pytorche_windows\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: ENTRA√éNEMENT AVEC EARLY STOPPING ET SAUVEGARDE\n",
    "print(\"üöÄ D√©marrage de l'entra√Ænement avec MobileNetV3...\")\n",
    "\n",
    "# Fonction de perte avec lissage des labels\n",
    "criterion = LabelSmoothCrossEntropyLoss(smoothing=config.label_smoothing)\n",
    "\n",
    "# Optimizer avec weight decay (uniquement les param√®tres entra√Ænables)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)\n",
    "\n",
    "# Early Stopping am√©lior√©\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=0.002, restore_best_weights=True)\n",
    "\n",
    "# Historique d'entra√Ænement\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"\\nüéØ D√©marrage de l'entra√Ænement pour {config.num_epochs} epochs...\")\n",
    "print(f\"‚è∞ Early Stopping: Patience = {early_stopping.patience} epochs\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nüìç Epoch {epoch+1}/{config.num_epochs}\")\n",
    "    \n",
    "    # Phase d'entra√Ænement\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Phase de validation\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Mise √† jour de l'historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    print(f\"  Train ‚Üí Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Val   ‚Üí Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # V√©rification Early Stopping\n",
    "    if early_stopping(val_loss, val_acc, model, epoch+1):\n",
    "        print(f\"\\nüõë Early Stopping d√©clench√© √† l'epoch {epoch + 1}!\")\n",
    "        break\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le (backup suppl√©mentaire)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history\n",
    "        }, 'best_mobilenetv3_model_complete.pth')\n",
    "        print(f\"  üíæ Mod√®le complet sauvegard√©! (Accuracy: {val_acc:.2f}%)\")\n",
    "    \n",
    "    # Ajustement du learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Restauration du meilleur mod√®le\n",
    "early_stopping.restore_best_model(model)\n",
    "\n",
    "print(f\"\\nüéâ Entra√Ænement termin√©!\")\n",
    "print(f\"üèÜ Meilleure accuracy de validation: {best_val_acc:.2f}% √† l'epoch {best_epoch}\")\n",
    "\n",
    "# Sauvegarde finale du mod√®le restaur√©\n",
    "torch.save(model.state_dict(), 'final_mobilenetv3_model.pth')\n",
    "print(\"üíæ Mod√®le final sauvegard√©!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: √âvaluation et m√©triques d√©taill√©es\n",
    "print(\"üìä √âvaluation du mod√®le final...\")\n",
    "\n",
    "# √âvaluation sur le test set\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f\"üéØ Performance sur le test set:\")\n",
    "print(f\"   - Loss: {test_loss:.4f}\")\n",
    "print(f\"   - Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# M√©triques d√©taill√©es\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Calcul des m√©triques\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "precision = precision_score(all_targets, all_preds, average='macro')\n",
    "recall = recall_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "print(f\"\\nüìà M√©triques d√©taill√©es:\")\n",
    "print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   - F1-Score: {f1:.4f}\")\n",
    "print(f\"   - Precision: {precision:.4f}\")\n",
    "print(f\"   - Recall: {recall:.4f}\")\n",
    "\n",
    "# Rapport de classification d√©taill√©\n",
    "print(f\"\\nüìã Rapport de classification:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=train_dataset.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Export ONNX du meilleur mod√®le\n",
    "print(\"\\nüì§ Export du mod√®le en format ONNX...\")\n",
    "\n",
    "# Assurer que le mod√®le est en mode √©valuation\n",
    "model.eval()\n",
    "\n",
    "# Cr√©er un exemple d'input\n",
    "dummy_input = torch.randn(1, 3, config.n_mels, config.target_length // config.hop_length + 1).to(device)\n",
    "\n",
    "# Export ONNX\n",
    "onnx_path = \"mobilenetv3_speech_commands.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    },\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le export√© avec succ√®s: {onnx_path}\")\n",
    "print(f\"üìä Taille du fichier ONNX: {os.path.getsize(onnx_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# V√©rification de l'export ONNX\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"‚úÖ Mod√®le ONNX v√©rifi√© avec succ√®s!\")\n",
    "    \n",
    "    # Test avec ONNX Runtime\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Test de pr√©diction\n",
    "    dummy_np = dummy_input.cpu().numpy()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_np}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    print(\"‚úÖ ONNX Runtime test r√©ussi!\")\n",
    "    print(f\"üìã Informations ONNX:\")\n",
    "    print(f\"   - Input shape: {ort_session.get_inputs()[0].shape}\")\n",
    "    print(f\"   - Output shape: {ort_session.get_outputs()[0].shape}\")\n",
    "    print(f\"   - Opset version: {onnx_model.opset_import[0].version}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ONNX non install√©, impossible de v√©rifier le mod√®le\")\n",
    "    print(\"üí° Installer avec: pip install onnx onnxruntime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d715cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualisations et sauvegarde finale\n",
    "print(\"\\nüé® Cr√©ation des visualisations...\")\n",
    "\n",
    "# Graphiques de performance\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2, alpha=0.8)\n",
    "plt.plot(history['val_loss'], label='Val Loss', linewidth=2, alpha=0.8)\n",
    "plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "plt.title('√âvolution des Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy', linewidth=2, alpha=0.8)\n",
    "plt.plot(history['val_acc'], label='Val Accuracy', linewidth=2, alpha=0.8)\n",
    "plt.axvline(x=best_epoch-1, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "plt.title('√âvolution de l\\'Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['learning_rates'], label='Learning Rate', linewidth=2, color='purple', alpha=0.8)\n",
    "plt.title('√âvolution du Learning Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics_mobilenetv3.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Matrice de Confusion - MobileNetV3\\nAccuracy: {test_acc:.2f}%')\n",
    "plt.colorbar()\n",
    "\n",
    "# Afficher seulement quelques labels pour la lisibilit√©\n",
    "if len(train_dataset.labels) > 20:\n",
    "    tick_marks = np.arange(0, len(train_dataset.labels), max(1, len(train_dataset.labels)//20))\n",
    "    plt.xticks(tick_marks, [train_dataset.labels[i] for i in tick_marks], rotation=45)\n",
    "    plt.yticks(tick_marks, [train_dataset.labels[i] for i in tick_marks])\n",
    "else:\n",
    "    tick_marks = np.arange(len(train_dataset.labels))\n",
    "    plt.xticks(tick_marks, train_dataset.labels, rotation=45)\n",
    "    plt.yticks(tick_marks, train_dataset.labels)\n",
    "\n",
    "plt.xlabel('Pr√©diction')\n",
    "plt.ylabel('V√©rit√© terrain')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_mobilenetv3.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarde de l'historique\n",
    "import json\n",
    "history_serializable = {k: [float(x) if isinstance(x, (np.floating, float)) else x for x in v] \n",
    "                       for k, v in history.items()}\n",
    "\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_serializable, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Toutes les op√©rations sont termin√©es!\")\n",
    "print(\"üìÅ Fichiers cr√©√©s:\")\n",
    "print(f\"   - best_mobilenetv3_model_complete.pth (mod√®le complet avec historique)\")\n",
    "print(f\"   - final_mobilenetv3_model.pth (mod√®le final)\")\n",
    "print(f\"   - mobilenetv3_speech_commands.onnx (mod√®le ONNX)\")\n",
    "print(f\"   - training_metrics_mobilenetv3.png (graphiques)\")\n",
    "print(f\"   - confusion_matrix_mobilenetv3.png (matrice de confusion)\")\n",
    "print(f\"   - training_history.json (historique d'entra√Ænement)\")\n",
    "\n",
    "print(f\"\\nüéØ R√©sultats finaux:\")\n",
    "print(f\"   - Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   - Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   - Best Epoch: {best_epoch}\")\n",
    "print(f\"   - Total Epochs: {epoch + 1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorche_windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
